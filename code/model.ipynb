{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disease Extraction (NER problem) \n",
    "https://datahack.analyticsvidhya.com/contest/innoplexus-online-hiring-hackathon-saving-lives-wi/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs:  4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import namedtuple\n",
    "from itertools import repeat\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import spacy\n",
    "import scispacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import gc\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "N_CORES = multiprocessing.cpu_count()\n",
    "print('Number of CPUs: ', N_CORES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_named_entities(tokens): # Helper Function for score calculation\n",
    "    \"\"\"\n",
    "    Creates a list of Entity named-tuples, storing the entity type and the start and end\n",
    "    offsets of the entity.\n",
    "    :param tokens: a list of labels\n",
    "    :return: a list of Entity named-tuples\n",
    "    \"\"\"\n",
    "    Entity = namedtuple(\"Entity\", \"e_type start_offset end_offset\")\n",
    "    named_entities = []\n",
    "    start_offset = None\n",
    "    end_offset = None\n",
    "    ent_type = None\n",
    "\n",
    "    for offset, token_tag in enumerate(tokens):\n",
    "\n",
    "        if token_tag == 'O':\n",
    "            if ent_type is not None and start_offset is not None:\n",
    "                end_offset = offset - 1\n",
    "                named_entities.append(Entity(ent_type, start_offset, end_offset))\n",
    "                start_offset = None\n",
    "                end_offset = None\n",
    "                ent_type = None\n",
    "\n",
    "        elif ent_type is None:\n",
    "            ent_type = token_tag[2:]\n",
    "            start_offset = offset\n",
    "\n",
    "        elif ent_type != token_tag[2:] or (ent_type == token_tag[2:] and token_tag[:1] == 'B'):\n",
    "\n",
    "            end_offset = offset - 1\n",
    "            named_entities.append(Entity(ent_type, start_offset, end_offset))\n",
    "\n",
    "            # start of a new entity\n",
    "            ent_type = token_tag[2:]\n",
    "            start_offset = offset\n",
    "            end_offset = None\n",
    "\n",
    "    # catches an entity that goes up until the last token\n",
    "    if ent_type and start_offset and end_offset is None:\n",
    "        named_entities.append(Entity(ent_type, start_offset, len(tokens)-1))\n",
    "\n",
    "    return named_entities\n",
    "\n",
    "def compute_metrics(true_named_entities, pred_named_entities): # Helper Function for score calculation\n",
    "    eval_metrics = {'correct': 0, 'partial': 0, 'missed': 0, 'spurius': 0}\n",
    "    target_tags_no_schema = ['indications']\n",
    "\n",
    "    # overall results\n",
    "    evaluation = {'partial': deepcopy(eval_metrics)}\n",
    "\n",
    "\n",
    "    true_which_overlapped_with_pred = []  # keep track of entities that overlapped\n",
    "\n",
    "    # go through each predicted named-entity\n",
    "    for pred in pred_named_entities:\n",
    "        found_overlap = False\n",
    "\n",
    "        # check if there's an exact match, i.e.: boundary and entity type match\n",
    "        if pred in true_named_entities:\n",
    "            true_which_overlapped_with_pred.append(pred)\n",
    "            evaluation['partial']['correct'] += 1\n",
    "\n",
    "        else:\n",
    "\n",
    "            # check for overlaps with any of the true entities\n",
    "            for true in true_named_entities:\n",
    "\n",
    "                \n",
    "                # 2. check for an overlap i.e. not exact boundary match, with true entities\n",
    "                if pred.start_offset <= true.end_offset and true.start_offset <= pred.end_offset:\n",
    "\n",
    "                    true_which_overlapped_with_pred.append(true)\n",
    "\n",
    "                    evaluation['partial']['partial'] += 1\n",
    "\n",
    "                    found_overlap = True\n",
    "                    break\n",
    "\n",
    "            # count spurius (i.e., False Positive) entities\n",
    "            if not found_overlap:\n",
    "                # overall results\n",
    "                evaluation['partial']['spurius'] += 1\n",
    "\n",
    "    # count missed entities (i.e. False Negative)\n",
    "    for true in true_named_entities:\n",
    "        if true in true_which_overlapped_with_pred:\n",
    "            continue\n",
    "        else:\n",
    "            # overall results\n",
    "            evaluation['partial']['missed'] += 1\n",
    "\n",
    "    # Compute 'possible', 'actual'\n",
    "    for eval_type in ['partial']:\n",
    "\n",
    "        correct = evaluation[eval_type]['correct']\n",
    "        partial = evaluation[eval_type]['partial']\n",
    "        missed = evaluation[eval_type]['missed']\n",
    "        spurius = evaluation[eval_type]['spurius']\n",
    "\n",
    "        # possible: nr. annotations in the gold-standard which contribute to the final score\n",
    "        evaluation[eval_type]['possible'] = correct + partial + missed\n",
    "\n",
    "        # actual: number of annotations produced by the NER system\n",
    "        evaluation[eval_type]['actual'] = correct + partial + spurius\n",
    "\n",
    "        actual = evaluation[eval_type]['actual']\n",
    "        possible = evaluation[eval_type]['possible']\n",
    "\n",
    "    return evaluation\n",
    "\n",
    "def list_converter(df): # Helper Function for score calculation\n",
    "    keys, values = df.sort_values('Sent_ID_x').values.T\n",
    "    ukeys, index = np.unique(keys,True)\n",
    "    lists = [list(array) for array in np.split(values,index[1:])]\n",
    "    return lists\n",
    "\n",
    "# ideal and pred respectively represent dataframes containing actual labels and predictions for the set of sentences in the test data. \n",
    "# It has the same format as the sample submission (id, Sent_ID, tag)\n",
    "\n",
    "def calculate_score(ideal, pred): # Calculates the final F1 Score\n",
    "\n",
    "    merged = ideal.merge(pred, on = \"id\", how=\"inner\").drop(['Sent_ID_y'],axis = 1)\n",
    "    \n",
    "    \n",
    "    # The scores are calculated sentence wise and then aggregated to calculate the overall score, for this\n",
    "    # List converter function groups the labels by sentence to generate a list of lists with each inner list representing a sentence in sequence\n",
    "    ideal_ = list_converter(merged.drop(['id','tag_y'],axis = 1))\n",
    "    pred_ = list_converter(merged.drop(['id','tag_x'],axis = 1))\n",
    "\n",
    "    metrics_results = {'correct': 0, 'partial': 0,\n",
    "                   'missed': 0, 'spurius': 0, 'possible': 0, 'actual': 0}\n",
    "\n",
    "    results = {'partial': deepcopy(metrics_results)}\n",
    "\n",
    "\n",
    "    for true_ents, pred_ents in zip(ideal_, pred_):    \n",
    "    # compute results for one sentence\n",
    "        tmp_results = compute_metrics(collect_named_entities(true_ents),collect_named_entities(pred_ents))\n",
    "    \n",
    "    # aggregate overall results\n",
    "        for eval_schema in results.keys():\n",
    "            for metric in metrics_results.keys():\n",
    "                results[eval_schema][metric] += tmp_results[eval_schema][metric]\n",
    "    correct = results['partial']['correct']\n",
    "    partial = results['partial']['partial']\n",
    "    missed = results['partial']['missed']\n",
    "    spurius = results['partial']['spurius']\n",
    "    actual = results['partial']['actual']\n",
    "    possible = results['partial']['possible']\n",
    "\n",
    "\n",
    "    precision = (correct + 0.5 * partial) / actual if actual > 0 else 0\n",
    "    recall = (correct + 0.5 * partial) / possible if possible > 0 else 0\n",
    "\n",
    "\n",
    "    score = (2 * precision * recall)/(precision + recall) if (precision + recall) >0 else 0\n",
    "    \n",
    "    # final score\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data and analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Doc_ID</th>\n",
       "      <th>Sent_ID</th>\n",
       "      <th>Word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Obesity</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Low-</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>and</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Middle-Income</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Doc_ID  Sent_ID           Word tag\n",
       "0   1       1        1        Obesity   O\n",
       "1   2       1        1             in   O\n",
       "2   3       1        1           Low-   O\n",
       "3   4       1        1            and   O\n",
       "4   5       1        1  Middle-Income   O"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4543833, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Doc_ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191282"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Sent_ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4543833"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have total 30000 documents and 191282 sentances in total.\n",
    "\n",
    "We have total 4543833 unique id, which means each record is assigned unique id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sci-spacy demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_ner_bc5cdr_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Obesity in Low- and Middle-Income Countries : Burden , Drivers , and Emerging Challenges . We have reviewed the distinctive features of excess weight , its causes , and related prevention and management efforts , as well as data gaps and recommendations for future research in low- and middle-income countries ( LMICs ) . Obesity is rising in every region of the world , and no country has been successful at reversing the epidemic once it has begun . In LMICs , overweight is higher in women compared with men , in urban compared with rural settings , and in older compared with younger individuals ; however , the urban-rural overweight differential is shrinking in many countries . Overweight occurs alongside persistent burdens of underweight in LMICs , especially in young women . Changes in the global diet and physical activity are among the hypothesized leading contributors to obesity . Emerging risk factors include environmental contaminants , chronic psychosocial stress , neuroendocrine dysregulation , and genetic/epigenetic mechanisms . Data on effective strategies to prevent the onset of obesity in LMICs or elsewhere are limited . Expanding the research in this area is a key priority and has important possibilities for reverse innovation that may also inform interventions in high-income countries . \"\"\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Obesity', 0, 7, 'DISEASE'),\n",
       " ('Obesity', 322, 329, 'DISEASE'),\n",
       " ('LMICs', 455, 460, 'DISEASE'),\n",
       " ('LMICs', 750, 755, 'DISEASE'),\n",
       " ('obesity', 886, 893, 'DISEASE'),\n",
       " ('neuroendocrine dysregulation', 985, 1013, 'DISEASE'),\n",
       " ('obesity', 1105, 1112, 'DISEASE'),\n",
       " ('LMICs', 1116, 1121, 'DISEASE')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(X, X.ent_iob_, X.ent_type_) for X in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spacy NER prediction dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create train and validation set.\n",
    "\n",
    "We are doing this based on if a doc has any entity in it or not.\n",
    "\n",
    "We will create stratified train and test split based on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ent = train_df.groupby('Doc_ID')['tag'].apply(lambda x: 'B-indications' in x.values).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_ID</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Doc_ID    tag\n",
       "0       1   True\n",
       "1       2   True\n",
       "2       3   True\n",
       "3       4  False\n",
       "4       5  False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_ent.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get train Doc ids and validation Doc ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_doc_ids, tst_doc_ids = train_test_split(has_ent['Doc_ID'].values, test_size = 0.33, stratify=has_ent['tag'].values, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20100  training documents.\n"
     ]
    }
   ],
   "source": [
    "print(len(trn_doc_ids), \" training documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9900  test documents.\n"
     ]
    }
   ],
   "source": [
    "print(len(tst_doc_ids), \" test documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = train_df[train_df['Doc_ID'].isin(tst_doc_ids)].reset_index(drop=True)\n",
    "train_df = train_df[train_df['Doc_ID'].isin(trn_doc_ids)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train our model on document level and also predict on document level, that's why we need to group data by Doc_ID.\n",
    "\n",
    "The reason behid this is that in training out model can use context words and learn better.\n",
    "\n",
    "Same goes for prediction. If we just single word for prediction, then model doesn't know its context words, and model can't predict better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main problem we face in using using spacy for prediction is that it takes whole document, do tokenization on its own and give us IOB (inside, outside, begining) prediction on tokens it has generated. Here we might face mismatch between tokens of spacy and our own, we need IOB predictions on our own tokens.\n",
    "\n",
    "That's why we have created a function which gives use prediction based on word location in document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creating test data for prediction\n",
    "def gb_ops_test(df):\n",
    "    ids = df['id'].tolist()\n",
    "    st_inds = []\n",
    "    doc = \"\"\n",
    "    ls_ind = 0\n",
    "    for w in df['Word']:\n",
    "        st_inds.append(ls_ind)\n",
    "        w_len = len(str(w))\n",
    "        ls_ind = ls_ind + w_len + 1\n",
    "        doc = doc + str(w) + \" \"\n",
    "    return pd.Series(dict(ids = ids, st_inds = st_inds, doc = doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_df.groupby('Doc_ID').apply(gb_ops_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>st_inds</th>\n",
       "      <th>doc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[212, 213]</td>\n",
       "      <td>[0, 14]</td>\n",
       "      <td>MICROCEPHALIA VERA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[214, 215, 216, 217, 218, 219, 220, 221, 222, ...</td>\n",
       "      <td>[0, 10, 26, 29, 35, 43, 52, 60, 63, 70, 75, 89...</td>\n",
       "      <td>Excellent reproducibility of laser speckle con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[519, 520, 521, 522, 523, 524, 525, 526, 527, ...</td>\n",
       "      <td>[0, 9, 19, 26, 29, 44, 47, 51, 63, 68, 74]</td>\n",
       "      <td>Positive inotropic action of cholinesterase on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[830, 831, 832, 833, 834, 835, 836, 837, 838, ...</td>\n",
       "      <td>[0, 15, 20, 29, 37, 39, 44, 46, 48, 51, 60, 65...</td>\n",
       "      <td>Self-assembled drug delivery systems . Part 8 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[1066, 1067, 1068, 1069, 1070, 1071, 1072, 107...</td>\n",
       "      <td>[0, 12, 16, 23, 34, 37, 44, 54, 57, 77, 87, 92...</td>\n",
       "      <td>Hyperphagia and leptin resistance in tissue in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      ids  \\\n",
       "Doc_ID                                                      \n",
       "2                                              [212, 213]   \n",
       "3       [214, 215, 216, 217, 218, 219, 220, 221, 222, ...   \n",
       "4       [519, 520, 521, 522, 523, 524, 525, 526, 527, ...   \n",
       "6       [830, 831, 832, 833, 834, 835, 836, 837, 838, ...   \n",
       "7       [1066, 1067, 1068, 1069, 1070, 1071, 1072, 107...   \n",
       "\n",
       "                                                  st_inds  \\\n",
       "Doc_ID                                                      \n",
       "2                                                 [0, 14]   \n",
       "3       [0, 10, 26, 29, 35, 43, 52, 60, 63, 70, 75, 89...   \n",
       "4              [0, 9, 19, 26, 29, 44, 47, 51, 63, 68, 74]   \n",
       "6       [0, 15, 20, 29, 37, 39, 44, 46, 48, 51, 60, 65...   \n",
       "7       [0, 12, 16, 23, 34, 37, 44, 54, 57, 77, 87, 92...   \n",
       "\n",
       "                                                      doc  \n",
       "Doc_ID                                                     \n",
       "2                                     MICROCEPHALIA VERA   \n",
       "3       Excellent reproducibility of laser speckle con...  \n",
       "4       Positive inotropic action of cholinesterase on...  \n",
       "6       Self-assembled drug delivery systems . Part 8 ...  \n",
       "7       Hyperphagia and leptin resistance in tissue in...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, ids contains word ids in doc, st_inds has starting index of all words which are in doc. and doc is text document.\n",
    "\n",
    "ids and st_inds has samesize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a benchmark model using pre trained Sci Spacy 'en_ner_bc5cdr_md' model\n",
    "\n",
    "Which is a NER for DISEASE and CHEMICHAL entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for prediction on document and returns 'B' and 'I' word ids in our dataset\n",
    "\n",
    "def get_tag(ids, st_inds, doc, nlp_obj):\n",
    "    ids = pd.Series(ids)\n",
    "    st_inds = pd.Series(st_inds)\n",
    "\n",
    "    doc = nlp_obj(str(doc))\n",
    "    out = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "\n",
    "    ans = {'B': [], 'I': []}\n",
    "\n",
    "    for o in out:\n",
    "        if o[3] == 'DISEASE':\n",
    "            inss = st_inds[(st_inds >= o[1]) & (st_inds <= o[2])].index.tolist()\n",
    "            if (st_inds == o[1]).sum() == 0:\n",
    "                inss = [(st_inds[st_inds > o[1]].index[0] - 1)] + inss\n",
    "            w_ids = ids.iloc[inss].tolist()\n",
    "            ans['B'].append(w_ids[0])\n",
    "            ans['I'].extend(w_ids[1:])\n",
    "    return ans\n",
    "\n",
    "def get_B_I_ids(temp_data):\n",
    "    B_pred_ids = []\n",
    "    I_pred_ids = []\n",
    "    for d in temp_data:\n",
    "        B_pred_ids.extend(d['B'])\n",
    "        I_pred_ids.extend(d['I'])\n",
    "        \n",
    "    return B_pred_ids, I_pred_ids\n",
    "\n",
    "def make_prediction(test_df, B_pred_ids, I_pred_ids):\n",
    "    ans = test_df[['id', 'Sent_ID']].copy()\n",
    "    ans['tag'] = 'O'\n",
    "    ans.loc[ans['id'].isin(B_pred_ids), 'tag'] = 'B-indications'\n",
    "    ans.loc[ans['id'].isin(I_pred_ids), 'tag'] = 'I-indications'\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.6307554562886555  minutes\n"
     ]
    }
   ],
   "source": [
    "# spacy NLP object\n",
    "nlp = spacy.load(\"en_ner_bc5cdr_md\")\n",
    "\n",
    "# function to put nlp object in get_tag function\n",
    "def mp_get_tag(ids, st_inds, doc):\n",
    "    return get_tag(ids, st_inds, doc, nlp)\n",
    "\n",
    "## Multiprocess code\n",
    "t1 = time.time()\n",
    "p = Pool(N_CORES)\n",
    "temp_data = list(p.starmap(mp_get_tag, zip(list(test_data['ids'].values), list(test_data['st_inds'].values), list(test_data['doc'].values))))\n",
    "p.close()\n",
    "p.join()\n",
    "p.terminate()\n",
    "print((time.time() - t1)/60, \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_pred_ids, I_pred_ids = get_B_I_ids(temp_data)\n",
    "test_pred_bm = make_prediction(test_df, B_pred_ids, I_pred_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Sent_ID</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>212</td>\n",
       "      <td>10</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>213</td>\n",
       "      <td>10</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>214</td>\n",
       "      <td>11</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>215</td>\n",
       "      <td>11</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>216</td>\n",
       "      <td>11</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  Sent_ID tag\n",
       "0  212       10   O\n",
       "1  213       10   O\n",
       "2  214       11   O\n",
       "3  215       11   O\n",
       "4  216       11   O"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_bm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.489518489355064"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_score(test_df[['id', 'Sent_ID', 'tag']], test_pred_bm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got 0.489518489355064 score on validation set using sci spacy pre trained model.\n",
    "\n",
    "On leader board highest score is 0.82.\n",
    "\n",
    "We can Update (further train) this sci spacy model on our train dataset to increase score upto 0.80 and even higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spacy train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trn_data(df):\n",
    "    ent_pres = False\n",
    "    doc = \"\"\n",
    "    ents = []\n",
    "    new_ind = 0\n",
    "    \n",
    "    for i, r in df.iterrows():\n",
    "        w_len = len(str(r['Word']))\n",
    "        doc = doc + str(r['Word']) + \" \"\n",
    "        \n",
    "        if r['tag'] == 'O':\n",
    "            new_ind = new_ind + w_len + 1\n",
    "            \n",
    "        if r['tag'] == 'B-indications':\n",
    "            st = new_ind\n",
    "            en = st + w_len\n",
    "            ents.append((st, en, 'DISEASE'))\n",
    "            new_ind = en + 1\n",
    "            \n",
    "        if r['tag'] == 'I-indications':\n",
    "            en = new_ind + w_len\n",
    "            st = ents[-1][0]\n",
    "            ents = ents[0:-1]\n",
    "            ents.append((st, en, 'DISEASE'))\n",
    "            new_ind = en + 1\n",
    "    \n",
    "    if ents:\n",
    "        ent_pres = True\n",
    "    \n",
    "    out = (doc, {\"entities\": ents})\n",
    "    return pd.Series(dict(trn_data = out, ent_pres = ent_pres))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_df.groupby('Sent_ID').apply(create_trn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trn_data</th>\n",
       "      <th>ent_pres</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sent_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Obesity in Low- and Middle-Income Countries :...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(We have reviewed the distinctive features of ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Obesity is rising in every region of the worl...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(In LMICs , overweight is higher in women comp...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(Overweight occurs alongside persistent burden...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  trn_data  ent_pres\n",
       "Sent_ID                                                             \n",
       "1        (Obesity in Low- and Middle-Income Countries :...     False\n",
       "2        (We have reviewed the distinctive features of ...     False\n",
       "3        (Obesity is rising in every region of the worl...     False\n",
       "4        (In LMICs , overweight is higher in women comp...     False\n",
       "5        (Overweight occurs alongside persistent burden...     False"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Obesity in Low- and Middle-Income Countries : Burden , Drivers , and Emerging Challenges . ',\n",
       " {'entities': []})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.iloc[0]['trn_data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train sci spacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trn_model(model= None, output_dir=None, n_iter=10):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        # reset and initialize the weights randomly â€“ but only if we're\n",
    "        # training a new model\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in tqdm(batches):\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.3,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses\", losses)\n",
    "    \n",
    "            # save model to output directory\n",
    "            if output_dir is not None:\n",
    "                output_dir = Path(output_dir)\n",
    "                if not output_dir.exists():\n",
    "                    output_dir.mkdir()\n",
    "                nlp.to_disk(output_dir)\n",
    "                print(\"Saved model to\", output_dir)\n",
    "\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = train_data['trn_data'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"en_ner_bc5cdr_md\"\n",
    "OUT_DIR = \"./model_final/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'en_ner_bc5cdr_md'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ad16cd707a43a5bfdb7ef6fd79753d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 129.74318817221916}\n",
      "Saved model to model_final\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c0b947c99b4366a6bd8e1a8e46ea5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 86.04977100593487}\n",
      "Saved model to model_final\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b090e4da5e94017b3be145b8929e1d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 74.31878978241406}\n",
      "Saved model to model_final\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca931eeb15124a95bd0018fabe668c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 61.91225983773926}\n",
      "Saved model to model_final\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc583e716d5147809d4df16bce0cc44d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 57.74447276394376}\n",
      "Saved model to model_final\n"
     ]
    }
   ],
   "source": [
    "nlp1 = trn_model(MODEL, OUT_DIR, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's predict on test set using this new updated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.970260135332743  minutes\n"
     ]
    }
   ],
   "source": [
    "# function to put nlp object in get_tag function\n",
    "def mp_get_tag(ids, st_inds, doc):\n",
    "    return get_tag(ids, st_inds, doc, nlp1)\n",
    "\n",
    "## Multiprocess code\n",
    "t1 = time.time()\n",
    "p = Pool(N_CORES)\n",
    "temp_data = list(p.starmap(mp_get_tag, zip(list(test_data['ids'].values), list(test_data['st_inds'].values), list(test_data['doc'].values))))\n",
    "p.close()\n",
    "p.join()\n",
    "p.terminate()\n",
    "print((time.time() - t1)/60, \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_pred_ids, I_pred_ids = get_B_I_ids(temp_data)\n",
    "test_pred_final = make_prediction(test_df, B_pred_ids, I_pred_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Sent_ID</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>212</td>\n",
       "      <td>10</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>213</td>\n",
       "      <td>10</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>214</td>\n",
       "      <td>11</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>215</td>\n",
       "      <td>11</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>216</td>\n",
       "      <td>11</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  Sent_ID tag\n",
       "0  212       10   O\n",
       "1  213       10   O\n",
       "2  214       11   O\n",
       "3  215       11   O\n",
       "4  216       11   O"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O                1466397\n",
       "B-indications      16878\n",
       "I-indications      13501\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_final['tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7855966520062774"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_score(test_df[['id', 'Sent_ID', 'tag']], test_pred_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We got a huge improvement over base pre trained model.\n",
    "\n",
    "We can train for few more epoch to increase the score.\n",
    "\n",
    "Just load this trained model and train it for few more epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model score : 0.7855966520062774"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
